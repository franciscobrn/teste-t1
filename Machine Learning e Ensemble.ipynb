{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdf64f47",
   "metadata": {},
   "source": [
    "# Machine Learning e Ensemble \n",
    "por [Anderson França](https://www.andersonfranca.me/)\n",
    "\n",
    "Os modelos ensemble são uma técnica de aprendizado de máquina que consiste em combinar vários modelos individuais para produzir uma previsão mais precisa do que um único modelo. Em vez de confiar em um único modelo para tomar uma decisão, o ensemble utiliza a sabedoria coletiva de vários modelos diferentes para produzir uma previsão mais precisa e robusta.\n",
    "\n",
    "Existem vários tipos de modelos ensemble, incluindo:\n",
    "\n",
    "- **Bagging:** utiliza modelos diferentes treinados em diferentes subconjuntos aleatórios dos dados de treinamento.\n",
    "\n",
    "- **Boosting:** treina modelos sequencialmente, dando mais peso aos exemplos de dados que foram mal previstos pelo modelo anterior.\n",
    "\n",
    "\n",
    "Os modelos ensemble são frequentemente usados em competições de ciência de dados e em problemas de aprendizado de máquina em que a precisão é crítica, como detecção de fraudes, previsão de demanda e previsão de preços de ações."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10faf6ce",
   "metadata": {},
   "source": [
    "- AdaBoost: diferente do que vimos nos dois algoritmos anteriores, no AdaBoost não teremos a construção de árvores de decisões, mas sim de “tocos”, ou stumps. Estes tocos são como árvores de decisão com apenas um nó, sendo que a construção de cada um dependerá do toco anterior. Ou seja, existe uma dependência entre cada toco, situação que não acontece nas árvores do RandomForest e do ExtraTrees. Esta é uma característica dos algoritmos de boosting, que utilizam o resultado de um modelo para criação do próximo, buscando um aperfeiçoamento a cada iteração, aprendendo com os erros do modelo anterior.\n",
    "\n",
    "- GradientBoosting: um dos mais poderosos algoritmos de boosting. Ele cria árvores decisões com o objetivo de prever o valor dos erros da árvore anterior, e utiliza o valor do erro previsto na definição de seu resultado final.\n",
    "\n",
    "- Bagging: seu principal objetivo é evitar o overfitting, através da criação de diferentes modelos de machine learning, utilizando como resultado final a média das respostas encontradas. O Bagging nos permite escolher qualquer algoritmo de machine learning para realizar este processo. Ou seja, poderemos utilizar Regressão Linear, Knn, Árvore de Decisão, e assim por diante, sendo que muitos modelos serão criados com o algoritmo escolhido. Cada modelo criado será diferente dos demais, pois serão escolhidas amostras aleatórias para criação destes modelos, não sendo utilizada a totalidade dos dados de treino em um único modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd402223",
   "metadata": {},
   "source": [
    "### Bagging "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ff5908",
   "metadata": {},
   "source": [
    "Modelos Bagging (Bootstrap Aggregating) são um tipo de técnica ensemble que envolve a combinação de múltiplos modelos de aprendizado de máquina treinados em amostras aleatórias dos dados de treinamento.\n",
    "\n",
    "O processo de bagging envolve a criação de várias amostras de bootstrap dos dados de treinamento. Em seguida, um modelo é treinado em cada amostra. Os modelos individuais são treinados independentemente uns dos outros, e suas previsões são combinadas para produzir uma previsão final.\n",
    "\n",
    "O objetivo do bagging é reduzir a variância dos modelos individuais, o que ajuda a melhorar a precisão e a estabilidade da previsão. Ao treinar cada modelo em um conjunto de dados diferente, o bagging pode ajudar a reduzir o _overfitting_ e aumentar a generalização do modelo.\n",
    "\n",
    "\n",
    "O Bagging pode ser usado com vários algoritmos de aprendizado de máquina, incluindo árvores de decisão, SVMs (Support Vector Machines), redes neurais e outros. É particularmente útil para reduzir o overfitting em modelos complexos, aumentando a estabilidade e a precisão das previsões.\n",
    "\n",
    "Alguns dos benefícios do Bagging incluem:\n",
    "\n",
    "- Redução do overfitting, tornando o modelo mais geral e robusto;\n",
    "- Aumento da estabilidade das previsões;\n",
    "- Possibilidade de avaliar a importância de cada variável de entrada no modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce75154",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "\n",
    "O Random Forest é um algoritmo do tipo ensemble que usa um conjunto de árvores de decisão para fazer previsões. O modelo de Random Forest é uma extensão da técnica de Bagging (Bootstrap Aggregating), que envolve a criação de múltiplos modelos de aprendizado de máquina treinados em subconjuntos aleatórios dos dados de treinamento. \n",
    "\n",
    "\n",
    "A criação de cada árvore de decisão no modelo segue o mesmo processo da criação de uma árvore de decisão em um modelo individual. A diferença é que, no caso do Random Forest, o processo é repetido várias vezes para criar várias árvores de decisão que são combinadas para gerar uma previsão final.\n",
    "\n",
    "Cada árvore de decisão no modelo é construída a partir de um subconjunto aleatório dos dados de treinamento e um subconjunto aleatório dos recursos (variáveis). Esse processo de amostragem aleatória ajuda a reduzir a correlação entre as árvores de decisão, tornando o modelo de Random Forest menos propenso a _overfitting_ do que um único modelo de árvore de decisão.\n",
    "\n",
    "Para fazer uma previsão usando um modelo de Random Forest, as previsões de cada árvore de decisão no modelo são combinadas para produzir uma previsão final. As árvores de decisão podem ser combinadas usando média ou votação, dependendo do tipo de problema que está sendo resolvido.\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1ZQnOvzm6KS5VjlnGP9H0Bu-1J97KqjLo\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "Imagem: [Random Forest Simplified](https://en.wikipedia.org/wiki/Random_forest)\n",
    "\n",
    "\n",
    "O modelo de Random Forest é frequentemente usado para problemas de classificação e regressão e é popular devido à sua facilidade de uso, robustez, e alta precisão de previsão. Para problemas de classificação, cada árvore de decisão no modelo faz uma previsão sobre a classe de saída, e a **classe mais frequente é escolhida** como a previsão final do modelo. Para problemas de regressão, cada árvore de decisão no modelo faz uma previsão sobre o valor de saída, e a **média das previsões** é usada como a previsão final do modelo.\n",
    "\n",
    "\n",
    "Podemos citar como vantagens do random forest a:\n",
    "\n",
    "- Robustez a dados ausentes ou ruidosos;\n",
    "- Tolerância a overfitting;\n",
    "- Possibilidade de avaliar a importância de cada variável de entrada no modelo;\n",
    "- Alta precisão em previsões.\n",
    "\n",
    "\n",
    "É importante notar que o modelo de Random Forest pode ser computacionalmente intensivo, especialmente com grandes bases de dados. Além disso, a interpretabilidade do modelo pode ser limitada devido à complexidade das árvores de decisão individuais no modelo.\n",
    "\n",
    "\n",
    "Para treinar o modelo, é necessário seguir as etapas de modelagem:\n",
    "- Tratar e selecionar as melhores variáveis\n",
    "- Dividir a base de dados\n",
    "- Treinar o modelos\n",
    "- Ajustar os hiperparâmetros\n",
    "- Avaliar a performance\n",
    "\n",
    "\n",
    "No Python, o modelo de Random Forest pode ser implementado usando a biblioteca scikit-learn. A classe **RandomForestClassifier** é usada para problemas de classificação e a classe **RandomForestRegressor** é usada para problemas de regressão."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f8dbe9",
   "metadata": {},
   "source": [
    "sklearn.ensemble.**RandomForestClassifier**(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "Confira a definição completa dos hiperparâmetros em: [Scikitlearn - Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fe1919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carregar base de Dados\n",
    "import pandas as pd\n",
    "base = pd.read_excel('Base Dados.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0af7c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame(base[['VAL_SH','VAL_SP','QT_DIARIAS','DIAR_ACOM']])\n",
    "y = base['MORTE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18c71650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d15e119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Particionar base de dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81892790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir parâmetros do modelo\n",
    "modelo = RandomForestClassifier(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d9119e",
   "metadata": {},
   "source": [
    "Os principais hiperparâmetros que podem ser ajustados no modelo de Random Forest são:\n",
    "\n",
    "- **n_estimators:** O número de árvores de decisão que serão criadas no modelo. Quanto mais árvores, mais estável e robusto é o modelo, mas também pode levar a um aumento no tempo de treinamento.\n",
    "\n",
    "- **max_depth:** A profundidade máxima da árvore de decisão. Controla a complexidade do modelo. Uma profundidade muito grande pode levar a overfitting, enquanto uma profundidade muito baixa pode levar ao _underfitting_.\n",
    "\n",
    "- **max_features:** O número máximo de recursos (variáveis) considerados para cada divisão da árvore. Um valor menor pode reduzir a correlação entre as árvores, mas também pode reduzir a precisão do modelo.\n",
    "\n",
    "- **min_samples_split:** O número mínimo de amostras necessárias para dividir um nó interno da árvore. Um valor muito pequeno pode levar a overfitting, enquanto um valor muito grande pode levar a underfitting.\n",
    "\n",
    "- **min_samples_leaf:** O número mínimo de amostras necessárias para serem consideradas em uma folha da árvore. Controla a complexidade da árvore. Valores muito baixos podem levar a overfitting, enquanto valores muito altos podem levar a underfitting.\n",
    "\n",
    "- **bootstrap:** Controla se as amostras são retiradas do conjunto de treinamento com ou sem reposição. Quando definido como \"True\", as amostras são retiradas com reposição, o que aumenta a variabilidade do modelo e pode levar a uma melhor precisão.\n",
    "\n",
    "- **Random_state:** Define a semente aleatória para garantir que os resultados sejam reproduzíveis. O modelo sempre produzirá os mesmos resultados se tiver um valor definido de estado aleatório e receber os mesmos hiperparâmetros e dados de treinamento.\n",
    "\n",
    "- **n_jobs:** informa ao mecanismo quantos processadores ele pode usar. Se o valor for 1, pode usar apenas um processador, mas se o valor for -1, não há limite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a68836e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ajustar o modelo\n",
    "modelo.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a704bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = modelo.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03c1a653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94      3541\n",
      "           1       0.50      0.26      0.34       459\n",
      "\n",
      "    accuracy                           0.88      4000\n",
      "   macro avg       0.70      0.61      0.64      4000\n",
      "weighted avg       0.86      0.88      0.87      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ca24f4",
   "metadata": {},
   "source": [
    "Ao ajustar um modelo de Random Forest, é importante ajustar os hiperparâmetros do modelo, como o número de árvores, a profundidade máxima da árvore e o número máximo de recursos usados para cada divisão da árvore, para obter a melhor precisão possível. A biblioteca Scikit-learn fornece funções úteis, como GridSearchCV, para encontrar os melhores hiperparâmetros automaticamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cdbee3",
   "metadata": {},
   "source": [
    "### ExtraTrees\n",
    "\n",
    "O ExtraTrees (Extremely Randomized Trees) é um algoritmo que utiliza a técnica de bagging para construir várias árvores de decisão aleatórias independentes. Ele é uma variação do algoritmo Random Forest, porém com algumas diferenças no processo de construção das árvores.\n",
    "\n",
    "Enquanto no Random Forest o algoritmo utiliza um subconjunto aleatório de características (variáveis) para cada nó da árvore durante o processo de construção, o ExtraTrees utiliza um subconjunto aleatório de características em cada nó e, além disso, seleciona aleatoriamente os pontos de divisão nos dados para cada subconjunto.\n",
    "\n",
    "Essa abordagem aleatória de seleção de pontos de divisão faz com que o ExtraTrees seja ainda mais resistente ao overfitting do que o Random Forest. Além disso, o processo de treinamento é mais rápido, uma vez que não é necessário calcular a medida de impureza para cada ponto de divisão.\n",
    "\n",
    "Assim como no Random Forest, o resultado final do ExtraTrees é uma média das previsões de cada árvore individual. O algoritmo é bastante versátil e pode ser utilizado em problemas de classificação e regressão.\n",
    "\n",
    "Uma desvantagem do ExtraTrees em relação ao Random Forest é que, por ser mais aleatório, ele pode ter uma tendência a gerar modelos menos interpretáveis. No entanto, em muitos casos, o aumento da precisão compensa essa desvantagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf05f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carregar biblioteca ExtraTreesClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88da3f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o modelo ExtraTreesClassifier\n",
    "modelo = ExtraTreesClassifier(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f223bbf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(random_state=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treina o modelo com o conjunto de treinamento\n",
    "modelo.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93f463e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz as previsões com o conjunto de teste\n",
    "y_pred = modelo.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fbff8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.93      3541\n",
      "           1       0.47      0.27      0.35       459\n",
      "\n",
      "    accuracy                           0.88      4000\n",
      "   macro avg       0.69      0.62      0.64      4000\n",
      "weighted avg       0.86      0.88      0.87      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb3d694",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2008b56b",
   "metadata": {},
   "source": [
    "o **Feature importance**, ou importância das variáveis, é uma medida que indica a importância relativa de cada característica ou variável de entrada para o modelo. De forma simples, o _feature importance_ mostra quais características tiveram mais influência na tomada de decisão do modelo.\n",
    "\n",
    "Existem diferentes maneiras de calcular o feature importance, dependendo do algoritmo utilizado. No caso do ExtraTrees, RandomForest e outros algoritmos baseados em árvores de decisão, uma das medidas mais comuns é o Gini Importance, que calcula a redução média de impureza (ou ganho de informação) que cada característica fornece ao modelo ao ser utilizada em cada nó da árvore. As características que fornecem maior ganho de informação têm uma importância maior para o modelo.\n",
    "\n",
    "A utilização da _feature importance_ em modelos como o Random Forest, pode trazer diversas vantagens:\n",
    "\n",
    "- **Seleção de features:** A partir da importância das features, podemos identificar as que mais influenciam na predição do modelo. Isso nos ajuda a selecionar as features mais importantes e reduzir o conjunto de features para um modelo mais simples e eficiente.\n",
    "\n",
    "- **Interpretabilidade:** A feature importance pode ser usada para entender quais são as características mais importantes para a tomada de decisões do modelo, o que pode ser útil para explicar o modelo para terceiros e para realizar análises exploratórias dos dados.\n",
    "\n",
    "- **Detecção de ruídos:** A feature importance pode ajudar a detectar variáveis que estão adicionando ruído ao modelo, ou seja, que estão atrapalhando a capacidade do modelo de generalizar os dados. Com essa informação, podemos remover ou reajustar essas features para melhorar o desempenho do modelo.\n",
    "\n",
    "- **Seleção de modelos:** A feature importance também pode ser usada para comparar o desempenho de diferentes modelos e identificar qual modelo é o mais adequado para um determinado conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e854f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "importancia_bd = pd.DataFrame({\n",
    "    'feature': modelo.feature_names_in_,\n",
    "    'importancia': modelo.feature_importances_\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "200270a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importancia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VAL_SH</td>\n",
       "      <td>0.440342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VAL_SP</td>\n",
       "      <td>0.349919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QT_DIARIAS</td>\n",
       "      <td>0.153460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DIAR_ACOM</td>\n",
       "      <td>0.056278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature  importancia\n",
       "0      VAL_SH     0.440342\n",
       "1      VAL_SP     0.349919\n",
       "2  QT_DIARIAS     0.153460\n",
       "3   DIAR_ACOM     0.056278"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ordenar o dataframe pela importância das features\n",
    "importancia_bd = importancia_bd.sort_values('importancia', ascending=False)\n",
    "importancia_bd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3933d7",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "Boosting é uma técnica que consiste em construir um modelo forte a partir de vários modelos fracos. A ideia central do boosting é treinar uma sequência de modelos que são capazes de aprender com os erros dos modelos anteriores, de forma que o modelo final seja mais preciso e robusto.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1WRLxndSy7yEg5ZgI_JzqDcA3qZDGqo7h\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "Imagem: [Random Forest Algorithms](https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/)\n",
    "\n",
    "\n",
    "O processo de boosting começa com a construção de um modelo simples, que pode ser uma árvore de decisão ou um modelo linear, por exemplo. Em seguida, o modelo é treinado e os erros são analisados. A partir dos erros, um segundo modelo é construído, dando mais peso aos exemplos que foram classificados erroneamente pelo primeiro modelo. Esse processo é repetido várias vezes, com cada novo modelo sendo treinado para corrigir os erros dos modelos anteriores.\n",
    "\n",
    "O boosting é especialmente útil quando lidamos com problemas de classificação com dados desbalanceados, onde uma classe é muito mais frequente do que a outra. Nesses casos, um modelo simples pode ter dificuldade em detectar padrões na classe minoritária, e o boosting pode ajudar a melhorar o desempenho do modelo.\n",
    "\n",
    "Existem vários modelos de Boosting disponíveis, cada um com suas características próprias. Abaixo estão listados alguns dos principais modelos de Boosting:\n",
    "\n",
    "- **AdaBoost (Adaptive Boosting):** É o modelo de Boosting mais conhecido e utilizado. Ele ajusta pesos para cada exemplo de treinamento de acordo com a dificuldade de classificação, e atribui maior peso aos exemplos que foram classificados erroneamente. No final, o modelo é formado pela combinação ponderada dos modelos individuais.\n",
    "\n",
    "- **Gradient Boosting:** É um modelo de Boosting em que cada modelo sucessivo é treinado para corrigir os erros do modelo anterior, usando o gradiente da função de perda como orientação. Este modelo é amplamente utilizado em competições de ciência de dados, devido ao seu desempenho superior em muitos problemas.\n",
    "\n",
    "- **XGBoost (Extreme Gradient Boosting):** É uma implementação otimizada do Gradient Boosting que utiliza algumas técnicas para reduzir o tempo de treinamento e melhorar o desempenho, como o uso de aproximações de segundo grau para calcular o gradiente.\n",
    "\n",
    "- **LightGBM:** É outra implementação otimizada do Gradient Boosting, que utiliza uma estrutura de árvore diferente da utilizada no XGBoost para tornar o treinamento mais rápido e eficiente.\n",
    "\n",
    "- **CatBoost:** É um modelo de Boosting que utiliza algumas técnicas especiais para lidar com variáveis categóricas, que são comuns em muitos problemas do mundo real."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15684d80",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "\n",
    "O AdaBoost funciona ajustando um conjunto de pesos para cada exemplo de treinamento, com base em quão bem o modelo atual está classificando os exemplos. Na primeira iteração, todos os exemplos têm o mesmo peso. Em seguida, um modelo fraco é treinado em todo o conjunto de treinamento, e os pesos são ajustados para que os exemplos que foram classificados incorretamente tenham maior peso na próxima iteração. O processo é repetido várias vezes, adicionando modelos fracos sucessivos ao conjunto, e ajustando os pesos a cada iteração.\n",
    "\n",
    "No final, os modelos fracos são combinados em um modelo forte, atribuindo a cada um deles um peso, de acordo com sua precisão em relação aos pesos dos exemplos. O modelo final é então usado para fazer previsões em novos dados.\n",
    "\n",
    "O AdaBoost é uma técnica eficaz para melhorar a precisão de modelos fracos, e tem sido amplamente utilizado em problemas de classificação binária e multiclasse. Ele tem a vantagem de ser fácil de implementar e não requer muitos hiperparâmetros para serem ajustados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c07d481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "235c3148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um classificador base (modelo fraco)\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "19cba248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo AdaBoost\n",
    "ada_boost = AdaBoostClassifier(base_estimator=base_estimator, \n",
    "                               n_estimators=50, \n",
    "                               random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "08a369be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   random_state=0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treinando o modelo\n",
    "ada_boost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa7da114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo previsões no conjunto de teste\n",
    "y_pred = ada_boost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1f0a1748",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94      3541\n",
      "           1       0.63      0.12      0.20       459\n",
      "\n",
      "    accuracy                           0.89      4000\n",
      "   macro avg       0.76      0.56      0.57      4000\n",
      "weighted avg       0.87      0.89      0.86      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdab7b9",
   "metadata": {},
   "source": [
    "Para mais modelos disponíveis no Sklearn, acesse a documentação em: [Ensemble Methods](https://scikit-learn.org/stable/modules/ensemble.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
